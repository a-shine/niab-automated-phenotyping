@article{furbankPhenomicsTechnologiesRelieve2011,
  title = {Phenomics – Technologies to Relieve the Phenotyping Bottleneck},
  author = {Furbank, Robert T. and Tester, Mark},
  date = {2011-12},
  journaltitle = {Trends in Plant Science},
  shortjournal = {Trends in Plant Science},
  volume = {16},
  number = {12},
  pages = {635--644},
  issn = {13601385},
  doi = {10.1016/j.tplants.2011.09.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1360138511002093},
  urldate = {2024-04-18},
  langid = {english}
}

@incollection{braytonPhenotyping2018,
  title = {Phenotyping},
  booktitle = {Comparative {{Anatomy}} and {{Histology}}},
  author = {Brayton, Cory F. and Treuting, Piper M.},
  date = {2018},
  pages = {9--21},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-802900-8.00002-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128029008000026},
  urldate = {2024-05-21},
  isbn = {978-0-12-802900-8},
  langid = {english}
}

@article{grosskinskyPlantPhenomicsNeed2015,
  title = {Plant Phenomics and the Need for Physiological Phenotyping across Scales to Narrow the Genotype-to-Phenotype Knowledge Gap},
  author = {Großkinsky, Dominik K. and Svensgaard, Jesper and Christensen, Svend and Roitsch, Thomas},
  date = {2015-09},
  journaltitle = {Journal of Experimental Botany},
  shortjournal = {EXBOTJ},
  volume = {66},
  number = {18},
  pages = {5429--5440},
  issn = {0022-0957, 1460-2431},
  doi = {10.1093/jxb/erv345},
  url = {https://academic.oup.com/jxb/article-lookup/doi/10.1093/jxb/erv345},
  urldate = {2024-04-18},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/NDLGZUC9/Großkinsky et al. - 2015 - Plant phenomics and the need for physiological phenotyping across scales to narrow the genotype-to-p.pdf}
}

@article{houlePhenomicsNextChallenge2010,
  title = {Phenomics: The next Challenge},
  shorttitle = {Phenomics},
  author = {Houle, David and Govindaraju, Diddahally R. and Omholt, Stig},
  date = {2010-12},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  volume = {11},
  number = {12},
  pages = {855--866},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg2897},
  url = {https://www.nature.com/articles/nrg2897},
  urldate = {2024-04-18},
  langid = {english}
}

@article{narisettiDeepLearningBased2022,
  title = {Deep {{Learning Based Greenhouse Image Segmentation}} and {{Shoot Phenotyping}} ({{DeepShoot}})},
  author = {Narisetti, Narendra and Henke, Michael and Neumann, Kerstin and Stolzenburg, Frieder and Altmann, Thomas and Gladilin, Evgeny},
  date = {2022-07-13},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {13},
  pages = {906410},
  issn = {1664-462X},
  doi = {10.3389/fpls.2022.906410},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2022.906410/full},
  urldate = {2024-04-09},
  abstract = {Background               Automated analysis of large image data is highly demanded in high-throughput plant phenotyping. Due to large variability in optical plant appearance and experimental setups, advanced machine and deep learning techniques are required for automated detection and segmentation of plant structures in complex optical scenes.                                         Methods               Here, we present a GUI-based software tool (DeepShoot) for efficient, fully automated segmentation and quantitative analysis of greenhouse-grown shoots which is based on pre-trained U-net deep learning models of arabidopsis, maize, and wheat plant appearance in different rotational side- and top-views.                                         Results               Our experimental results show that the developed algorithmic framework performs automated segmentation of side- and top-view images of different shoots acquired at different developmental stages using different phenotyping facilities with an average accuracy of more than 90\% and outperforms shallow as well as conventional and encoder backbone networks in cross-validation tests with respect to both precision and performance time.                                         Conclusion               The DeepShoot tool presented in this study provides an efficient solution for automated segmentation and phenotypic characterization of greenhouse-grown plant shoots suitable also for end-users without advanced IT skills. Primarily trained on images of three selected plants, this tool can be applied to images of other plant species exhibiting similar optical properties.},
  keywords = {IMPORTANT},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/YMFZGKX6/Narisetti et al. - 2022 - Deep Learning Based Greenhouse Image Segmentation and Shoot Phenotyping (DeepShoot).pdf}
}

@article{liReviewImagingTechniques2014,
  title = {A {{Review}} of {{Imaging Techniques}} for {{Plant Phenotyping}}},
  author = {Li, Lei and Zhang, Qin and Huang, Danfeng},
  date = {2014-10-24},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {14},
  number = {11},
  pages = {20078--20111},
  issn = {1424-8220},
  doi = {10.3390/s141120078},
  url = {http://www.mdpi.com/1424-8220/14/11/20078},
  urldate = {2024-05-21},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/N4NSGCAI/Li et al. - 2014 - A Review of Imaging Techniques for Plant Phenotyping.pdf}
}

@incollection{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} – {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  date = {2015},
  volume = {9351},
  pages = {234--241},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
  urldate = {2024-05-13},
  isbn = {978-3-319-24573-7 978-3-319-24574-4},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/DURTFNCA/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf}
}

@article{redaAchievingFoodSecurity2014,
  title = {Achieving {{Food Security}} in {{Ethiopia}} by {{Promoting Productivity}} of {{Future World Food Tef}}: {{A Review}}},
  shorttitle = {Achieving {{Food Security}} in {{Ethiopia}} by {{Promoting Productivity}} of {{Future World Food Tef}}},
  author = {Reda, Abraham},
  date = {2014-04-09},
  journaltitle = {Advances in Plants \& Agriculture Research},
  shortjournal = {APAR},
  volume = {2},
  number = {2},
  issn = {23736402},
  doi = {10.15406/apar.2015.02.00045},
  url = {https://medcraveonline.com/APAR/achieving-food-security-in-ethiopia-by-promoting-productivity-of-future-world-food-tef-a-review.html},
  urldate = {2024-05-22}
}

@article{tadeleEmpiricalReviewUse2021,
  title = {Empirical Review on the Use Dynamics and Economics of Teff in {{Ethiopia}}},
  author = {Tadele, Esubalew and Hibistu, Tewabe},
  date = {2021-12},
  journaltitle = {Agriculture \& Food Security},
  shortjournal = {Agric \& Food Secur},
  volume = {10},
  number = {1},
  pages = {40},
  issn = {2048-7010},
  doi = {10.1186/s40066-021-00329-2},
  url = {https://agricultureandfoodsecurity.biomedcentral.com/articles/10.1186/s40066-021-00329-2},
  urldate = {2024-05-22},
  abstract = {Abstract                            Background               Teff is a warm-season cereal crop and the tiniest grain on the planet. It is one of the underutilized crops that can contribute to food security and crop diversification. It is nutritious and well adapted to the growing conditions in Ethiopia, but little has been invested to expand its potential to the domestic or international markets.                                         Method               Comprehensive empirical review was carried out emphasizing the spatial, temporal production dynamics of teff in Ethiopia and its economic value. Different inclusion and exclusion criteria were applied and filtered pertinent to this study in country-wide verdicts.                                         Results               Ethiopia is not only the biggest teff-producing nation but also the only nation to have adopted teff as a staple crop. Teff contains a high nutritive value and has unique dietary benefits due to its being gluten-free and is typically preferred by health-conscious consumers. However, teff producing and value addition practice is insufficient and generally depends on conventional practices, and its marketplace is restricted local and the government imposes an export ban on it to limit the upward pressure on domestic grain prices and address local food security. Instead, other countries, such as USA, are increasingly participating in the teff market and teff has a great contribution for foreign earning through Injera. Because of its appealing nutritional and functional features, the crop’s popularity is fast growing over the world. Several health advantages have been associated with the grain, these conditions necessitate extensive investigation on the grain’s nutritional and functional qualities.                                         Conclusion               This study examines the use dynamics and economics of teff in Ethiopia. Teff has received restricted consideration from the global market and mainstream researches perhaps due to its orphan crop’ status. To take comparative advantage of the growing domestic and international demand of teff, intensive investment by the domestic teff industry needs to improve methods of teff producing, opening up and expanding its international market to ensuring its super global food and part of the solution to worldwide food and nutrition security gains. Ethiopia should improve to take the lead in the growing teff market and support others to engage in teff food system part.},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/YBX4JVUZ/Tadele and Hibistu - 2021 - Empirical review on the use dynamics and economics of teff in Ethiopia.pdf}
}

@article{millerComputerVisionAnalysis2007,
  title = {Computer‐vision Analysis of Seedling Responses to Light and Gravity},
  author = {Miller, Nathan D. and Parks, Brian M. and Spalding, Edgar P.},
  date = {2007-10},
  journaltitle = {The Plant Journal},
  shortjournal = {The Plant Journal},
  volume = {52},
  number = {2},
  pages = {374--381},
  issn = {0960-7412, 1365-313X},
  doi = {10.1111/j.1365-313X.2007.03237.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1365-313X.2007.03237.x},
  urldate = {2024-05-22},
  abstract = {Summary                            Measuring the effects of mutation, natural variation or treatment on the development of plant form is often complicated by the shapes, dynamics or small size of the organismal structures under study. This limits accuracy and throughput of measurement and thereby limits progress toward understanding the underlying gene networks and signaling systems. A computer‐vision platform based on electronic image capture and shape‐analysis algorithms was developed as an alternative to the mostly manual methods of measuring seedling development currently in use. The spatial and temporal resolution of the method is in the range of microns and minutes, respectively. The algorithm simultaneously quantifies apical hook opening and inhibition of hypocotyl elongation during photomorphogenesis of               Arabidopsis thaliana               seedlings. It can determine when and where gravitropic curvature develops along the root axis in               A. thaliana               and               Medicago truncatula               seedlings. Novel features of gravitropic curvature development were discovered as a result of the high resolution. The computer‐vision algorithms developed and demonstrated here could be used to study mutant phenotypes in detail, to form the basis of a high‐throughput screening platform, or to quantify natural variation in a population of plants.},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/9JGBXIC6/Miller et al. - 2007 - Computer‐vision analysis of seedling responses to light and gravity.pdf}
}

@article{fahlgrenLightsCameraAction2015,
  title = {Lights, Camera, Action: High-Throughput Plant Phenotyping Is Ready for a Close-Up},
  shorttitle = {Lights, Camera, Action},
  author = {Fahlgren, Noah and Gehan, Malia A and Baxter, Ivan},
  date = {2015-04},
  journaltitle = {Current Opinion in Plant Biology},
  shortjournal = {Current Opinion in Plant Biology},
  volume = {24},
  pages = {93--99},
  issn = {13695266},
  doi = {10.1016/j.pbi.2015.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1369526615000266},
  urldate = {2024-05-22},
  langid = {english}
}

@article{minerviniImageAnalysisNew2015,
  title = {Image {{Analysis}}: {{The New Bottleneck}} in {{Plant Phenotyping}} [{{Applications Corner}}]},
  shorttitle = {Image {{Analysis}}},
  author = {Minervini, Massimo and Scharr, Hanno and Tsaftaris, Sotirios A.},
  date = {2015-07},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {32},
  number = {4},
  pages = {126--131},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2015.2405111},
  url = {https://ieeexplore.ieee.org/document/7123050/},
  urldate = {2024-05-22},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/EUAKDRDG/Minervini et al. - 2015 - Image Analysis The New Bottleneck in Plant Phenotyping [Applications Corner].pdf}
}

@article{smithSegmentationRootsSoil2020,
  title = {Segmentation of Roots in Soil with {{U-Net}}},
  author = {Smith, Abraham George and Petersen, Jens and Selvan, Raghavendra and Rasmussen, Camilla Ruø},
  date = {2020-12},
  journaltitle = {Plant Methods},
  shortjournal = {Plant Methods},
  volume = {16},
  number = {1},
  pages = {13},
  issn = {1746-4811},
  doi = {10.1186/s13007-020-0563-0},
  url = {https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-0563-0},
  urldate = {2024-03-27},
  abstract = {Abstract                            Background                                Plant root research can provide a way to attain stress-tolerant crops that produce greater yield in a diverse array of conditions. Phenotyping roots in soil is often challenging due to the roots being difficult to access and the use of time consuming manual methods. Rhizotrons allow visual inspection of root growth through transparent surfaces. Agronomists currently manually label photographs of roots obtained from rhizotrons using a line-intersect method to obtain root length density and rooting depth measurements which are essential for their experiments. We investigate the effectiveness of an automated image segmentation method based on the U-Net Convolutional Neural Network (CNN) architecture to enable such measurements. We design a data-set of 50 annotated chicory (                 Cichorium intybus                 L.) root images which we use to train, validate and test the system and compare against a baseline built using the Frangi vesselness filter. We obtain metrics using manual annotations and line-intersect counts.                                                        Results                                Our results on the held out data show our proposed automated segmentation system to be a viable solution for detecting and quantifying roots. We evaluate our system using 867 images for which we have obtained line-intersect counts, attaining a Spearman rank correlation of 0.9748 and an                                                         \$\$r\textasciicircum 2\$\$                                                                     r                         2                                                                                                 of 0.9217. We also achieve an                                                         \$\$F\_1\$\$                                                                     F                         1                                                                                                 of 0.7 when comparing the automated segmentation to the manual annotations, with our automated segmentation system producing segmentations with higher quality than the manual annotations for large portions of the image.                                                        Conclusion               We have demonstrated the feasibility of a U-Net based CNN system for segmenting images of roots in soil and for replacing the manual line-intersect method. The success of our approach is also a demonstration of the feasibility of deep learning in practice for small research groups needing to create their own custom labelled dataset from scratch.},
  langid = {english},
  keywords = {IMPORTANT},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/G597G4EI/Smith et al. - 2020 - Segmentation of roots in soil with U-Net.pdf}
}

@article{bodnerRGBSpectralRoot2017,
  title = {{{RGB}} and {{Spectral Root Imaging}} for {{Plant Phenotyping}} and {{Physiological Research}}: {{Experimental Setup}} and {{Imaging Protocols}}},
  shorttitle = {{{RGB}} and {{Spectral Root Imaging}} for {{Plant Phenotyping}} and {{Physiological Research}}},
  author = {Bodner, Gernot and Alsalem, Mouhannad and Nakhforoosh, Alireza and Arnold, Thomas and Leitner, Daniel},
  date = {2017-08-08},
  journaltitle = {Journal of Visualized Experiments},
  shortjournal = {JoVE},
  number = {126},
  pages = {56251},
  issn = {1940-087X},
  doi = {10.3791/56251},
  url = {https://www.jove.com/t/56251/rgb-spectral-root-imaging-for-plant-phenotyping-physiological},
  urldate = {2024-03-26},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/SGZZCPYJ/Bodner et al. - 2017 - RGB and Spectral Root Imaging for Plant Phenotyping and Physiological Research Experimental Setup a.pdf}
}

@inproceedings{chaudhuryComputerVisionBased2015,
  title = {Computer {{Vision Based Autonomous Robotic System}} for {{3D Plant Growth Measurement}}},
  booktitle = {2015 12th {{Conference}} on {{Computer}} and {{Robot Vision}}},
  author = {Chaudhury, Ayan and Ward, Christopher and Talasaz, Ali and Ivanov, Alexander G. and Huner, Norman P.A. and Grodzinski, Bernard and Patel, Rajni V. and Barron, John L.},
  date = {2015-06},
  pages = {290--296},
  publisher = {IEEE},
  location = {Halifax, NS, Canada},
  doi = {10.1109/CRV.2015.45},
  url = {http://ieeexplore.ieee.org/document/7158932/},
  urldate = {2024-03-26},
  eventtitle = {2015 12th {{Conference}} on {{Computer}} and {{Robot Vision}} ({{CRV}})},
  isbn = {978-1-4799-1986-4},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/5YIPZRTN/Chaudhury et al. - 2015 - Computer Vision Based Autonomous Robotic System for 3D Plant Growth Measurement.pdf}
}

@article{chaudhuryMachineVisionSystem2019,
  title = {Machine {{Vision System}} for {{3D Plant Phenotyping}}},
  author = {Chaudhury, Ayan and Ward, Christopher and Talasaz, Ali and Ivanov, Alexander G. and Brophy, Mark and Grodzinski, Bernard and Huner, Norman P. A. and Patel, Rajnikant V. and Barron, John L.},
  date = {2019-11-01},
  journaltitle = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  shortjournal = {IEEE/ACM Trans. Comput. Biol. and Bioinf.},
  volume = {16},
  number = {6},
  pages = {2009--2022},
  issn = {1545-5963, 1557-9964, 2374-0043},
  doi = {10.1109/TCBB.2018.2824814},
  url = {https://ieeexplore.ieee.org/document/8334629/},
  urldate = {2024-03-26},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/64QTJKKL/Chaudhury et al. - 2019 - Machine Vision System for 3D Plant Phenotyping.pdf}
}

@article{clarkThreeDimensionalRootPhenotyping2011,
  title = {Three-{{Dimensional Root Phenotyping}} with a {{Novel Imaging}} and {{Software Platform}}},
  author = {Clark, Randy T. and MacCurdy, Robert B. and Jung, Janelle K. and Shaff, Jon E. and McCouch, Susan R. and Aneshansley, Daniel J. and Kochian, Leon V.},
  date = {2011-06-01},
  journaltitle = {Plant Physiology},
  volume = {156},
  number = {2},
  pages = {455--465},
  issn = {1532-2548},
  doi = {10.1104/pp.110.169102},
  url = {https://academic.oup.com/plphys/article/156/2/455/6108789},
  urldate = {2024-03-24},
  abstract = {Abstract             A novel imaging and software platform was developed for the high-throughput phenotyping of three-dimensional root traits during seedling development. To demonstrate the platform’s capacity, plants of two rice (Oryza sativa) genotypes, Azucena and IR64, were grown in a transparent gellan gum system and imaged daily for 10 d. Rotational image sequences consisting of 40 two-dimensional images were captured using an optically corrected digital imaging system. Three-dimensional root reconstructions were generated and analyzed using a custom-designed software, RootReader3D. Using the automated and interactive capabilities of RootReader3D, five rice root types were classified and 27 phenotypic root traits were measured to characterize these two genotypes. Where possible, measurements from the three-dimensional platform were validated and were highly correlated with conventional two-dimensional measurements. When comparing gellan gum-grown plants with those grown under hydroponic and sand culture, significant differences were detected in morphological root traits (P \&lt; 0.05). This highly flexible platform provides the capacity to measure root traits with a high degree of spatial and temporal resolution and will facilitate novel investigations into the development of entire root systems or selected components of root systems. In combination with the extensive genetic resources that are now available, this platform will be a powerful resource to further explore the molecular and genetic determinants of root system architecture.},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/NJLYEXIK/Clark et al. - 2011 - Three-Dimensional Root Phenotyping with a Novel Imaging and Software Platform.pdf}
}

@article{daschoudhuryHolisticComponentPlant2018,
  title = {Holistic and Component Plant Phenotyping Using Temporal Image Sequence},
  author = {Das Choudhury, Sruti and Bashyam, Srinidhi and Qiu, Yumou and Samal, Ashok and Awada, Tala},
  date = {2018-12},
  journaltitle = {Plant Methods},
  shortjournal = {Plant Methods},
  volume = {14},
  number = {1},
  pages = {35},
  issn = {1746-4811},
  doi = {10.1186/s13007-018-0303-x},
  url = {https://plantmethods.biomedcentral.com/articles/10.1186/s13007-018-0303-x},
  urldate = {2024-04-18},
  langid = {english},
  keywords = {TOREAD},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/5IT8QJUE/Das Choudhury et al. - 2018 - Holistic and component plant phenotyping using temporal image sequence.pdf}
}

@article{fanSegmentationGuidedDeepLearning2022,
  title = {A {{Segmentation-Guided Deep Learning Framework}} for {{Leaf Counting}}},
  author = {Fan, Xijian and Zhou, Rui and Tjahjadi, Tardi and Das Choudhury, Sruti and Ye, Qiaolin},
  date = {2022-05-19},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {13},
  pages = {844522},
  issn = {1664-462X},
  doi = {10.3389/fpls.2022.844522},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2022.844522/full},
  urldate = {2024-04-18},
  abstract = {Deep learning-based methods have recently provided a means to rapidly and effectively extract various plant traits due to their powerful ability to depict a plant image across a variety of species and growth conditions. In this study, we focus on dealing with two fundamental tasks in plant phenotyping, i.e., plant segmentation and leaf counting, and propose a two-steam deep learning framework for segmenting plants and counting leaves with various size and shape from two-dimensional plant images. In the first stream, a multi-scale segmentation model using spatial pyramid is developed to extract leaves with different size and shape, where the fine-grained details of leaves are captured using deep feature extractor. In the second stream, a regression counting model is proposed to estimate the number of leaves without any pre-detection, where an auxiliary binary mask from segmentation stream is introduced to enhance the counting performance by effectively alleviating the influence of complex background. Extensive pot experiments are conducted CVPPP 2017 Leaf Counting Challenge dataset, which contains images of Arabidopsis and tobacco plants. The experimental results demonstrate that the proposed framework achieves a promising performance both in plant segmentation and leaf counting, providing a reference for the automatic analysis of plant phenotypes.},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/8F6HJCWR/Fan et al. - 2022 - A Segmentation-Guided Deep Learning Framework for Leaf Counting.pdf}
}

@article{finchWheatRootLength2017,
  title = {Wheat Root Length and Not Branching Is Altered in the Presence of Neighbours, Including Blackgrass},
  author = {Finch, Jessica A. and Guillaume, Gaëtan and French, Stephanie A. and Colaço, Renato D. D. R. and Davies, Julia M. and Swarbreck, Stéphanie M.},
  editor = {Aroca, Ricardo},
  date = {2017-05-24},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {12},
  number = {5},
  pages = {e0178176},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0178176},
  url = {https://dx.plos.org/10.1371/journal.pone.0178176},
  urldate = {2024-03-24},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/STKHBU3Y/Finch et al. - 2017 - Wheat root length and not branching is altered in the presence of neighbours, including blackgrass.pdf}
}

@article{guoIlluminationInvariantSegmentation2013,
  title = {Illumination Invariant Segmentation of Vegetation for Time Series Wheat Images Based on Decision Tree Model},
  author = {Guo, Wei and Rage, Uday K. and Ninomiya, Seishi},
  date = {2013-08},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {96},
  pages = {58--66},
  issn = {01681699},
  doi = {10.1016/j.compag.2013.04.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169913000847},
  urldate = {2024-04-10},
  langid = {english},
  keywords = {TOREAD}
}

@article{guoEasyPCCBenchmarkDatasets2017,
  title = {{{EasyPCC}}: {{Benchmark Datasets}} and {{Tools}} for {{High-Throughput Measurement}} of the {{Plant Canopy Coverage Ratio}} under {{Field Conditions}}},
  shorttitle = {{{EasyPCC}}},
  author = {Guo, Wei and Zheng, Bangyou and Duan, Tao and Fukatsu, Tokihiro and Chapman, Scott and Ninomiya, Seishi},
  date = {2017-04-07},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {17},
  number = {4},
  pages = {798},
  issn = {1424-8220},
  doi = {10.3390/s17040798},
  url = {http://www.mdpi.com/1424-8220/17/4/798},
  urldate = {2024-04-09},
  langid = {english},
  keywords = {IMPORTANT},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/AWNH7JVR/Guo et al. - 2017 - EasyPCC Benchmark Datasets and Tools for High-Throughput Measurement of the Plant Canopy Coverage R.pdf}
}

@article{henkeSemiAutomatedGroundTruth2021,
  title = {Semi-{{Automated Ground Truth Segmentation}} and {{Phenotyping}} of {{Plant Structures Using}} k-{{Means Clustering}} of {{Eigen-Colors}} ({{kmSeg}})},
  author = {Henke, Michael and Neumann, Kerstin and Altmann, Thomas and Gladilin, Evgeny},
  date = {2021-11-04},
  journaltitle = {Agriculture},
  shortjournal = {Agriculture},
  volume = {11},
  number = {11},
  pages = {1098},
  issn = {2077-0472},
  doi = {10.3390/agriculture11111098},
  url = {https://www.mdpi.com/2077-0472/11/11/1098},
  urldate = {2024-04-09},
  abstract = {Background. Efficient analysis of large image data produced in greenhouse phenotyping experiments is often challenged by a large variability of optical plant and background appearance which requires advanced classification model methods and reliable ground truth data for their training. In the absence of appropriate computational tools, generation of ground truth data has to be performed manually, which represents a time-consuming task. Methods. Here, we present a efficient GUI-based software solution which reduces the task of plant image segmentation to manual annotation of a small number of image regions automatically pre-segmented using k-means clustering of Eigen-colors (kmSeg). Results. Our experimental results show that in contrast to other supervised clustering techniques k-means enables a computationally efficient pre-segmentation of large plant images in their original resolution. Thereby, the binary segmentation of plant images in fore- and background regions is performed within a few minutes with the average accuracy of 96–99\% validated by a direct comparison with ground truth data. Conclusions. Primarily developed for efficient ground truth segmentation and phenotyping of greenhouse-grown plants, the kmSeg tool can be applied for efficient labeling and quantitative analysis of arbitrary images exhibiting distinctive differences between colors of fore- and background structures.},
  langid = {english},
  keywords = {TOREAD},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/IG5ZL4GS/Henke et al. - 2021 - Semi-Automated Ground Truth Segmentation and Phenotyping of Plant Structures Using k-Means Clusterin.pdf}
}

@article{hutherARADEEPOPSISAutomatedWorkflow2020,
  title = {{{ARADEEPOPSIS}}, an {{Automated Workflow}} for {{Top-View Plant Phenomics}} Using {{Semantic Segmentation}} of {{Leaf States}}},
  author = {Hüther, Patrick and Schandry, Niklas and Jandrasits, Katharina and Bezrukov, Ilja and Becker, Claude},
  date = {2020-12},
  journaltitle = {The Plant Cell},
  shortjournal = {Plant Cell},
  volume = {32},
  number = {12},
  pages = {3674--3688},
  issn = {1040-4651, 1532-298X},
  doi = {10.1105/tpc.20.00318},
  url = {https://academic.oup.com/plcell/article/32/12/3674-3688/6118590},
  urldate = {2024-04-29},
  langid = {english},
  keywords = {TOREAD},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/4XTWXNNA/Hüther et al. - 2020 - ARADEEPOPSIS, an Automated Workflow for Top-View Plant Phenomics using Semantic Segmentation of Leaf.pdf}
}

@article{leeAutomatedHighthroughputPlant2018,
  title = {An Automated, High-Throughput Plant Phenotyping System Using Machine Learning-Based Plant Segmentation and Image Analysis},
  author = {Lee, Unseok and Chang, Sungyul and Putra, Gian Anantrio and Kim, Hyoungseok and Kim, Dong Hwan},
  editor = {Candela, Hector},
  date = {2018-04-27},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {13},
  number = {4},
  pages = {e0196615},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0196615},
  url = {https://dx.plos.org/10.1371/journal.pone.0196615},
  urldate = {2024-04-10},
  langid = {english},
  keywords = {TOREAD},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/QQZQA47W/Lee et al. - 2018 - An automated, high-throughput plant phenotyping system using machine learning-based plant segmentati.pdf}
}

@article{luoEff3DPSeg3DOrganLevel2023,
  title = {Eff-{{3DPSeg}}: {{3D Organ-Level Plant Shoot Segmentation Using Annotation-Efficient Deep Learning}}},
  shorttitle = {Eff-{{3DPSeg}}},
  author = {Luo, Liyi and Jiang, Xintong and Yang, Yu and Samy, Eugene Roy Antony and Lefsrud, Mark and Hoyos-Villegas, Valerio and Sun, Shangpeng},
  date = {2023-01},
  journaltitle = {Plant Phenomics},
  shortjournal = {Plant Phenomics},
  volume = {5},
  pages = {0080},
  issn = {2643-6515},
  doi = {10.34133/plantphenomics.0080},
  url = {https://spj.science.org/doi/10.34133/plantphenomics.0080},
  urldate = {2024-04-09},
  abstract = {Reliable and automated 3-dimensional (3D) plant shoot segmentation is a core prerequisite for the extraction of plant phenotypic traits at the organ level. Combining deep learning and point clouds can provide effective ways to address the challenge. However, fully supervised deep learning methods require datasets to be point-wise annotated, which is extremely expensive and time-consuming. In our work, we proposed a novel weakly supervised framework, Eff-3DPSeg, for 3D plant shoot segmentation. First, high-resolution point clouds of soybean were reconstructed using a low-cost photogrammetry system, and the Meshlab-based Plant Annotator was developed for plant point cloud annotation. Second, a weakly supervised deep learning method was proposed for plant organ segmentation. The method contained (a) pretraining a self-supervised network using Viewpoint Bottleneck loss to learn meaningful intrinsic structure representation from the raw point clouds and (b) fine-tuning the pretrained model with about only 0.5\% points being annotated to implement plant organ segmentation. After, 3 phenotypic traits (stem diameter, leaf width, and leaf length) were extracted. To test the generality of the proposed method, the public dataset Pheno4D was included in this study. Experimental results showed that the weakly supervised network obtained similar segmentation performance compared with the fully supervised setting. Our method achieved 95.1\%, 96.6\%, 95.8\%, and 92.2\% in the precision, recall, F1 score, and mIoU for stem–leaf segmentation for the soybean dataset and 53\%, 62.8\%, and 70.3\% in the AP, AP@25, and AP@50 for leaf instance segmentation for the Pheno4D dataset. This study provides an effective way for characterizing 3D plant architecture, which will become useful for plant breeders to enhance selection processes. The trained networks are available at               https://github.com/jieyi-one/EFF-3DPSEG               .},
  langid = {english},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/GA5GCKUE/Luo et al. - 2023 - Eff-3DPSeg 3D Organ-Level Plant Shoot Segmentation Using Annotation-Efficient Deep Learning.pdf}
}

@incollection{muhlichMeasuringPlantRoot2008,
  title = {Measuring {{Plant Root Growth}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Mühlich, Matthias and Truhn, Daniel and Nagel, Kerstin and Walter, Achim and Scharr, Hanno and Aach, Til},
  editor = {Rigoll, Gerhard},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2008},
  volume = {5096},
  pages = {497--506},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-69321-5_50},
  url = {https://link.springer.com/10.1007/978-3-540-69321-5_50},
  urldate = {2024-03-26},
  isbn = {978-3-540-69320-8 978-3-540-69321-5},
  langid = {english},
  keywords = {IMPORTANT},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/FFZXB9HB/Mühlich et al. - 2008 - Measuring Plant Root Growth.pdf}
}

@article{yatesNewImagingMachinelearning2021,
  entrysubtype = {newspaper},
  title = {New Imaging, Machine-Learning Methods Speed Effort to Reduce Crops' Need for Water},
  author = {Yates, Diana},
  date = {2021-08-24},
  journaltitle = {University of Illinois News Bureau},
  url = {https://news.illinois.edu/view/6367/1803510789},
  urldate = {2024-03-24},
  abstract = {New imaging and machine learning (ML) tools developed by University of Illinois at Urbana-Champaign (U of I) scientists can analyze the genomic features of plant leaves as a means of increasing water-use efficiency for crops. The team analyzed lighter green pores (stomata) on leaves of corn, sorghum, and grasses of the genus Setaria to determine their role in water-use efficiency during photosynthesis. U of I's Jiayang Xie repurposed an ML tool designed to help driverless cars navigate complex environments into an application that could rapidly identify, count, and measure thousands of cells and cell features in each leaf sample. According to U of I’s Andrew Leakey, the researchers found “the size and shape of the stomata in corn appeared to be more important than had previously been recognized,” which will inform future efforts to breed crops that use water more efficiently.}
}

@article{wassonPortableFluorescenceSpectroscopy2016,
  title = {A Portable Fluorescence Spectroscopy Imaging System for Automated Root Phenotyping in Soil Cores in the Field},
  author = {Wasson, Anton and Bischof, Leanne and Zwart, Alec and Watt, Michelle},
  date = {2016-02},
  journaltitle = {Journal of Experimental Botany},
  shortjournal = {EXBOTJ},
  volume = {67},
  number = {4},
  pages = {1033--1043},
  issn = {0022-0957, 1460-2431},
  doi = {10.1093/jxb/erv570},
  url = {https://academic.oup.com/jxb/article-lookup/doi/10.1093/jxb/erv570},
  urldate = {2024-05-22},
  langid = {english}
}

@article{smithOotAinterDeep2022,
  title = {R {\textsc{Oot}} {{P}} {\textsc{Ainter}} : Deep Learning Segmentation of Biological Images with Corrective Annotation},
  shorttitle = {R},
  author = {Smith, Abraham George and Han, Eusun and Petersen, Jens and Olsen, Niels Alvin Faircloth and Giese, Christian and Athmann, Miriam and Dresbøll, Dorte Bodin and Thorup‐Kristensen, Kristian},
  date = {2022-10},
  journaltitle = {New Phytologist},
  shortjournal = {New Phytologist},
  volume = {236},
  number = {2},
  pages = {774--791},
  issn = {0028-646X, 1469-8137},
  doi = {10.1111/nph.18387},
  url = {https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.18387},
  urldate = {2024-05-22},
  abstract = {Summary                                                                                     Convolutional neural networks (CNNs) are a powerful tool for plant image analysis, but challenges remain in making them more accessible to researchers without a machine‐learning background. We present R                     oot                     P                     ainter                     , an open‐source graphical user interface based software tool for the rapid training of deep neural networks for use in biological image analysis.                                                                                             We evaluate R                     oot                     P                     ainter                     by training models for root length extraction from chicory (                     Cichorium intybus                     L.) roots in soil, biopore counting, and root nodule counting. We also compare dense annotations with corrective ones that are added during the training process based on the weaknesses of the current model.                                                                                             Five out of six times the models trained using R                     oot                     P                     ainter                     with corrective annotations created within 2\,h produced measurements strongly correlating with manual measurements. Model accuracy had a significant correlation with annotation duration, indicating further improvements could be obtained with extended annotation.                                                                                             Our results show that a deep‐learning model can be trained to a high accuracy for the three respective datasets of varying target objects, background, and image quality with {$<$}\,2\,h of annotation time. They indicate that, when using R                     oot                     P                     ainter                     , for many datasets it is possible to annotate, train, and complete data processing within 1\,d.},
  langid = {english}
}

@article{adamsPlantSegmentationSupervised2020,
  title = {Plant Segmentation by Supervised Machine Learning Methods},
  author = {Adams, Jason and Qiu, Yumou and Xu, Yuhang and Schnable, James C.},
  date = {2020-01},
  journaltitle = {The Plant Phenome Journal},
  shortjournal = {The Plant Phenome Journal},
  volume = {3},
  number = {1},
  pages = {e20001},
  issn = {2578-2703, 2578-2703},
  doi = {10.1002/ppj2.20001},
  url = {https://acsess.onlinelibrary.wiley.com/doi/10.1002/ppj2.20001},
  urldate = {2024-04-10},
  abstract = {Abstract             High‐throughput phenotyping systems provide abundant data for statistical analysis through plant imaging. Before usable data can be obtained, image processing must take place. In this study, we used supervised learning methods to segment plants from the background in such images and compared them with commonly used thresholding methods. Because obtaining accurate training data is a major obstacle to using supervised learning methods for segmentation, a novel approach to producing accurate labels was developed. We demonstrated that, with careful selection of training data through such an approach, supervised learning methods, and neural networks in particular, can outperform thresholding methods at segmentation.},
  langid = {english},
  keywords = {TOREAD},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/KUUSZJ2W/Adams et al. - 2020 - Plant segmentation by supervised machine learning methods.pdf}
}

@article{whiteGeneratingSegmentationMasks2020,
  title = {Generating Segmentation Masks of Herbarium Specimens and a Data Set for Training Segmentation Models Using Deep Learning},
  author = {White, Alexander E. and Dikow, Rebecca B. and Baugh, Makinnon and Jenkins, Abigail and Frandsen, Paul B.},
  date = {2020-06},
  journaltitle = {Applications in Plant Sciences},
  shortjournal = {Appl Plant Sci},
  volume = {8},
  number = {6},
  pages = {e11352},
  issn = {2168-0450, 2168-0450},
  doi = {10.1002/aps3.11352},
  url = {https://bsapubs.onlinelibrary.wiley.com/doi/10.1002/aps3.11352},
  urldate = {2024-04-18},
  abstract = {Premise               Digitized images of herbarium specimens are highly diverse with many potential sources of visual noise and bias. The systematic removal of noise and minimization of bias must be achieved in order to generate biological insights based on the plants rather than the digitization and mounting practices involved. Here, we develop a workflow and data set of high‐resolution image masks to segment plant tissues in herbarium specimen images and remove background pixels using deep learning.                                         Methods and Results               We generated 400 curated, high‐resolution masks of ferns using a combination of automatic and manual tools for image manipulation. We used those images to train a U‐Net‐style deep learning model for image segmentation, achieving a final Sørensen–Dice coefficient of 0.96. The resulting model can automatically, efficiently, and accurately segment massive data sets of digitized herbarium specimens, particularly for ferns.                                         Conclusions               The application of deep learning in herbarium sciences requires transparent and systematic protocols for generating training data so that these labor‐intensive resources can be generalized to other deep learning applications. Segmentation ground‐truth masks are hard‐won data, and we share these data and the model openly in the hopes of furthering model training and transfer learning opportunities for broader herbarium applications.},
  langid = {english},
  keywords = {TOREAD},
  file = {/Users/alex/Library/CloudStorage/GoogleDrive-a.shinebourne@gmail.com/My Drive/zotero/storage/YWS93H5V/White et al. - 2020 - Generating segmentation masks of herbarium specimens and a data set for training segmentation models.pdf}
}

@article{campilloUsingDigitalImages2008,
  title = {Using {{Digital Images}} to {{Characterize Canopy Coverage}} and {{Light Interception}} in a {{Processing Tomato Crop}}},
  author = {Campillo, C. and Prieto, M.H. and Daza, C. and Moñino, M.J. and García, M.I.},
  date = {2008-10},
  journaltitle = {HortScience},
  shortjournal = {horts},
  volume = {43},
  number = {6},
  pages = {1780--1786},
  issn = {0018-5345, 2327-9834},
  doi = {10.21273/HORTSCI.43.6.1780},
  url = {https://journals.ashs.org/view/journals/hortsci/43/6/article-p1780.xml},
  urldate = {2024-05-23},
  abstract = {Canopy light interception (LI) is a determining factor for crop growth and yield. Crop yield depends on a canopy's capacity to intercept incident solar radiation, which in turn depends on the available leaf area, its structure, and its efficiency in converting the energy captured by the plant into biomass. Digital images offer a series of advantages over other methods of LI estimation, including the possibility to directly process images by computer for which free software is available. The objectives of this work were to develop a simple, economical method for determining LI in low-lying crops such as processing tomato using digital images obtained with a standard, commercial camera and free software and to evaluate the influence of different types of soil coverage (bare soil and plastic mulch) on LI. Photographs of the selected areas were taken using a digital camera at a distance of 160 cm above the center of each area. The resulting digital images were then analyzed with the free software GIMP 2.2 and IMAGE J. Three methods [area (SA), contour (SC). and reclassification (SR)] were used to quantify the percentage of groundcover (PGC). They were applied to the same images and compared with LI as measured with a line quantum sensor at solar noon. There was a close relationship between LI and estimated PGC with all three methods and for different soil cover regimes. In all cases, there was a linear adjustment with a significant correlation coefficient (               P               {$<$} 0.01) and an               r               2               of greater than 0.88. The adjustment with RI was narrowest when the SR method was used to estimate PGC (               r               2               = 0.93) followed by SC (               r               2               = 0.92) and SA (               r               2               = 0.88). Measurements of LI based on digital images offered practical advantages with respect to the use of photosynthetically active radiation bars because the latter must be used at solar noon. In contrast, measurements obtained with a digital camera can be taken at any time of day and bright sunshine is not necessary. Different correlations were obtained for bare soil and plastic mulch conditions, so it was necessary to use a different equation to estimate LI under each condition.}
}

@article{casadesusUsingVegetationIndices2007,
  title = {Using Vegetation Indices Derived from Conventional Digital Cameras as Selection Criteria for Wheat Breeding in Water‐limited Environments},
  author = {Casadesús, J. and Kaya, Y. and Bort, J. and Nachit, M. M. and Araus, J. L. and Amor, S and Ferrazzano, G. and Maalouf, F. and Maccaferri, M. and Martos, V. and Ouabbou, H. and Villegas, D.},
  date = {2007-04},
  journaltitle = {Annals of Applied Biology},
  shortjournal = {Annals of Applied Biology},
  volume = {150},
  number = {2},
  pages = {227--236},
  issn = {0003-4746, 1744-7348},
  doi = {10.1111/j.1744-7348.2007.00116.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1744-7348.2007.00116.x},
  urldate = {2024-05-23},
  abstract = {Abstract             The ability to assess green biomass is of particular interest in a number of wheat breeding environments. However, the measurement of this and similar traits is either tedious and time‐consuming or requires the use of expensive, sophisticated equipment, such as field‐based spectroradiometers to measure vegetation indices (VIs). Here, conventional digital cameras are proposed as affordable and easy‐to‐use tools for gathering field data in wheat breeding programmes. Using appropriate software, a large set of images can be automatically processed to calculate a number of VIs, based on the performance of simple colour operations on each picture. The purpose of this study was to identify a set of picture‐derived vegetation indices (picVIs) and to evaluate their performance in durum wheat trials growing under rainfed and supplementary irrigation conditions. Here, zenithal pictures of each plot were obtained roughly 2{$\quad$}weeks after anthesis, and the picVIs that were calculated were compared with the normalised difference vegetation index (NDVI), an index derived from spectroradiometrical measurements, and with the grain yield (GY) from the same plots. The picVIs that performed best were the Hue, CIE‐Lab a* and CIE‐Luv u* components of the average colour of each picture, the relative green area (GA) and the ‘greener area’, similar to GA but excluding the more yellowish‐green pixels. Our results showed a high correlation between all these picVIs and the NDVI. Moreover, in rainfed conditions, each picVI provided an estimation of GY similar to or slightly better than that provided by the NDVI. However, in irrigated conditions during anthesis, neither these picVIs nor the NDVI provided a good estimation of GY, apparently because of the saturation of the VI response in conditions of complete soil cover and high plant density.},
  langid = {english}
}
